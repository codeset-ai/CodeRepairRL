# Model configuration
# For GRPO training, use the merged SFT model path instead of base model + LoRA checkpoint
# Example: model_name: "outputs/sft_model_merged" or "bjarni/qwen3-8b-swe-gym-sft-merged"
model_name: "Qwen/Qwen3-8B"
attn_implementation: "flash_attention_2"  #  flash attention 3 is only supported on >= H100

# LoRA configuration
# Having a lower rank LoRA is supported by literature, having low rank updates is actually beneficial to avoid overfitting / overspecializing
lora: true
r: 32
lora_alpha: 64 # lora paper describes 2x the r
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"